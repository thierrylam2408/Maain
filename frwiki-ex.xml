<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="fr">
  <siteinfo>
    <sitename>Wikipédia</sitename>
    <dbname>frwiki</dbname>
    <base>https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Accueil_principal</base>
    <generator>MediaWiki 1.33.0-wmf.13</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Média</namespace>
      <namespace key="-1" case="first-letter">Spécial</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Discussion</namespace>
      <namespace key="2" case="first-letter">Utilisateur</namespace>
      <namespace key="3" case="first-letter">Discussion utilisateur</namespace>
      <namespace key="4" case="first-letter">Wikipédia</namespace>
      <namespace key="5" case="first-letter">Discussion Wikipédia</namespace>
      <namespace key="6" case="first-letter">Fichier</namespace>
      <namespace key="7" case="first-letter">Discussion fichier</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">Discussion MediaWiki</namespace>
      <namespace key="10" case="first-letter">Modèle</namespace>
      <namespace key="11" case="first-letter">Discussion modèle</namespace>
      <namespace key="12" case="first-letter">Aide</namespace>
      <namespace key="13" case="first-letter">Discussion aide</namespace>
      <namespace key="14" case="first-letter">Catégorie</namespace>
      <namespace key="15" case="first-letter">Discussion catégorie</namespace>
      <namespace key="100" case="first-letter">Portail</namespace>
      <namespace key="101" case="first-letter">Discussion Portail</namespace>
      <namespace key="102" case="first-letter">Projet</namespace>
      <namespace key="103" case="first-letter">Discussion Projet</namespace>
      <namespace key="104" case="first-letter">Référence</namespace>
      <namespace key="105" case="first-letter">Discussion Référence</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Discussion module</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Discussion gadget</namespace>
      <namespace key="2302" case="case-sensitive">Définition de gadget</namespace>
      <namespace key="2303" case="case-sensitive">Discussion définition de gadget</namespace>
      <namespace key="2600" case="first-letter">Sujet</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Antoine Meillet</title>
    <ns>0</ns>
    <id>3</id>
    <revision>
      <id>154039698</id>
      <parentid>152515873</parentid>
      <timestamp>2018-11-17T21:47:05Z</timestamp>
      <contributor>
        <username>Nomen ad hoc</username>
        <id>1966288</id>
      </contributor>
      <minor />
      <comment>/* Liens externes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">
        <!--
        [[Algèbre linéaire|algebre]]
        [[Algorithmique|algo]]
        -->
        {{Infobox Linguiste
|nom                     = Antoine Meillet
|nationalité             = {{France}}
|date de naissance       = {{Date de naissance|11|novembre|1866}}
|lieu de naissance       = [[Moulins (Allier)|Moulins]] ([[Allier (département)|Allier]])
|date de décès           = {{Date de décès|21|septembre|1936|11|novembre|1866}}
|lieu de décès           = [[Châteaumeillant]]
|région                  = Linguiste occidental
|époque                  = {{s|XX}}
|image                   = Meillet Antoine.jpg
|légende                 =
|tradition linguistique  = [[Linguistique comparée]]
|principaux intérêts     =
|influencé par           =
|influence de            =
|idées remarquables      = [[épithète homérique]]
|œuvres principales      = ''Introduction à l'étude comparative des langues indo-européennes'' ([[1903]])&lt;br /&gt;
''Aperçu d'une histoire de la langue grecque'' ([[1913]])&lt;br /&gt;
''Dictionnaire étymologique de la langue latine'' ([[1932]])
|adjectifs dérivés       =
}}
'''Paul Jules Antoine Meillet''', né le {{Date de naissance|11|novembre|1866}} à [[Moulins (Allier)|Moulins]] ([[Allier (département)|Allier]]) et mort le {{Date de décès|21|septembre|1936}} à [[Châteaumeillant]] ([[Cher (département)|Cher]]), est le principal [[liste de linguistes|linguiste]] français des premières décennies du {{XXe siècle}}. Il est aussi [[philologue]].

== Biographie ==
D'origine bourbonnaise, fils d'un notaire de [[Châteaumeillant]] ([[Cher (département)|Cher]]), il fait ses études secondaires au [[lycée Théodore-de-Banville|lycée]] de [[Moulins (Allier)|Moulins]].

Étudiant à la [[faculté des lettres de Paris]] à partir de [[1885]] où il suit notamment les cours de [[Louis Havet]], il assiste également à ceux de [[Michel Bréal]] au [[Collège de France]] et de [[Ferdinand de Saussure]] à l'[[École pratique des hautes études]]. Il assure à la suite de Saussure le cours de [[grammaire comparée]], qu'il complète à partir de 1894 par une conférence sur l'[[Peuples iraniens|iranien]].

En 1897, il soutient sa thèse pour le [[doctorat]] ès lettres ''(Recherches sur l'emploi du génitif-accusatif en vieux-slave)''. En 1905, il occupe la chaire de grammaire comparée au [[Collège de France]], où il consacre ses cours à l'histoire et à la structure des [[langues indo-européennes]]. Il succéda au linguiste [[Auguste Carrière]] à la tête de la chaire d'[[arménien]] à l'[[Institut national des langues et civilisations orientales|École des langues orientales]]&lt;ref&gt;[http://www.inalco.fr/ina_gabarit_rubrique.php3?ctx=langue&amp;id_rubrique=47&amp;id_departement=8&amp;ina_rubrique_departement=1193&amp;id_langue=8&amp;ina_rubrique_langue=918&amp;ina_rubrique_2=1699 Institut National des Langues et Civilisations Orientales]&lt;/ref&gt;.

Secrétaire de la [[Société de linguistique de Paris]], il est élu à l'[[Académie des inscriptions et belles-lettres]] en 1924.

Il a formé toute une génération de linguistes français, parmi lesquels [[Émile Benveniste]], [[Marcel Cohen]], [[Georges Dumézil]], [[André Martinet]], [[Aurélien Sauvageot]], [[Lucien Tesnière]], [[Joseph Vendryes]], ainsi que le japonisant [[Charles Haguenauer]]. Antoine Meillet devait diriger la thèse de [[Jean Paulhan]] sur la sémantique du proverbe et c'est lui qui découvrit [[Gustave Guillaume]].

Il a influencé aussi un certain nombre de linguistes étrangers. Il a également été le premier à identifier le phénomène de la [[grammaticalisation]].

== Études arméniennes ==
* [[1890]], une mission de trois mois dans le [[Caucase]] lui permet d'apprendre l'[[arménien]] moderne.
* [[1902]], il obtient la [[chaire d'arménien de l'École des langues orientales]].
* [[1903]], nouvelle mission en Arménie russe, il publie son ''Esquisse d'une grammaire comparée de l'arménien classique'', qui demeure une référence en linguistique arménienne et indo-européenne jusqu'à ce jour. L'un de ses étudiants, [[Hratchia Adjarian]], devient le fondateur de la dialectologie arménienne. C'est également sous les encouragements de Meillet qu'Émile Benveniste étudie la langue arménienne.
* [[1919]], il est cofondateur de la Société des études arméniennes avec Victor Bérard, Charles Diehl, André-Ferdinand Hérold, H. Lacroix, Frédéric Macler, Gabriel Millet, Gustave Schlumberger.
* [[1920]], le 19 janvier, il crée la ''[[Revue des études arméniennes]]'' avec [[Frédéric Macler]].

== Études homériques ==
À la Sorbonne, Meillet surveille le travail de [[Milman Parry]]. En 1923, un an avant que Parry ne commence son travail avec Meillet, celui-ci écrit (cité dans la première des deux thèses de Milman Parry, à savoir celle qui traite de l'[[épithète homérique]]) :

{{citation bloc|L'épopée homérique est entièrement composée de formules, transmise de poète en poète. Un examen de n'importe quel passage révélera vite qu'il est fait de vers et de fragments de vers qui sont reproduits mot pour mot dans un ou dans plusieurs autres passages. Et même des vers, dont les parties ne se retrouvent pas dans un autre passage, ont le caractère d'une formule, et c'est sans aucun doute par un pur hasard qu'ils ne sont pas attestés ailleurs.}}

Meillet offre à son étudiant l'opinion, nouvelle à cette époque, que la structure formulaïque de ''[[l'Iliade]]'' serait une conséquence directe de sa transmission orale. Ainsi, il le dirige vers l'étude de l'oralité dans son cadre natif et lui suggère d'observer les mécanismes d'une tradition orale vivante à côté du texte classique (''[[l'Iliade]]'') qui est censé résulter d'une telle tradition. En conséquence, Meillet présente Parry à [[Matija Murko]], savant originaire de [[Slovénie]] qui avait longuement écrit sur la tradition héroïque épique dans les [[Balkans]], surtout en [[Bosnie-Herzégovine]]&lt;ref&gt;[[Mathias Murko]], ''La poésie populaire épique en Yougoslavie au début du {{s-|XX|e}}'' (Paris: Champion, 1929); Albert Lord, ''The singer of tales'' (Cambridge, Mass.: Harvard University Press, 1960), {{p.|11-12}}; Andrew Dalby, ''Rediscovering Homer'' (New York, London: Norton, 2006. {{ISBN|0-393-05788-7}}), {{p.|186-187}}.&lt;/ref&gt;. Par leurs recherches, dont les résultats sont à présent hébergés par l'université de Harvard, Parry et son élève, [[Albert Lord]], ont profondément renouvelé les études homériques.

== Principaux ouvrages ==
* ''Esquisse d'une grammaire comparée de l'arménien classique'', [[1903]].
* ''Introduction à l'étude comparative des langues indo-européennes'', [[1903]] ({{1re}} éd.), Hachette, Paris, [[1912]] ({{3e}} éd.)&lt;ref&gt;Cet ouvrage, ainsi que l{{'}}''Aperçu d'une histoire de la langue grecque'' ont fait l'objet d'une critique par [[Lucien Febvre]], ''Antoine Meillet et l'histoire, La Grèce ancienne à travers l'histoire'', Revue de synthèse historique, 1913, {{p.|4-93}}, rééditée dans Lucien Febvre, ''Vivre l'histoire'', coll. Bouquins, Robert Laffont/Armand Colin, Paris, 2009, {{p.|136-145}}.&lt;/ref&gt;.
* ''Les dialectes indo-européens'', [[1908]].
* ''Aperçu d'une histoire de la langue grecque'', [[1913]].
* ''Altarmenisches Elementarbuch'', [[1913]]. Heidelberg (en français : Manuel élémentaire d'Arménien classique, traduction de Gabriel Képéklian, Limoges, Lambert-Lucas, 2017 {{ISBN|978-2-35935-094-4}})
* ''Linguistique historique et linguistique générale'', [[1921]] (le tome II est paru en 1936 ; les deux tomes ont été réunis chez Lambert-Lucas, Limoges, 2015).
* ''Les origines indo-européennes des mètres grecs'', [[1923]].
* ''Traité de grammaire comparée des langues classiques'', [[1924]] (avec Joseph Vendryés).
* ''La méthode comparative en linguistique historique'', [[1925]], Oslo, Instituttet for Sammenlignende Kulturforskning (réimpr. Paris, Champion, 1954).
* {{Ouvrage|langue=fr|titre=Esquisse d'une histoire de la langue latine|éditeur= Klincksieck|lieu= Paris|année= 1977|isbn=2-252-01871-2}}.
* ''Dictionnaire étymologique de la langue latine'', [[1932]] (en collab. Avec [[Alfred Ernout]] (1879-1973), éd. augmentée, par Jacques André (1910-1994), Paris : Klincksieck, 2001, {{ISBN|2-252-03359-2}} {{BNF|37707942}}
* ''Meillet en Arménie, 1891, 1903'', Journaux et lettres publiés par Francis Gandon, Limoges, Lambert-Lucas, 2014, {{ISBN|978-2-35935-071-5}}.

== Notes et références ==
&lt;references /&gt;

== Voir aussi ==
{{Autres projets
|wikisource = Antoine Meillet
|commons = Category:Antoine Meillet
}}
=== Bibliographie ===
* {{Article|langue=fr|prénom1=Anne-Marguerite|nom1=Fryba|titre=[[Maurice Grammont]], Antoine Meillet et l'institutionnalisation de la linguistique en France|périodique=Revue des langues romanes|numéro=105|année=2001|pages=503-517}}
* {{Chapitre|langue=fr|prénom1=Charles|nom1=de Lamberterie|lien auteur1=Charles de Lamberterie|titre chapitre=Milman Parry et Antoine Meillet|auteurs ouvrage=Françoise Létoublon (éd.)|titre ouvrage=Hommage à [[Milman Parry]]. Le style formulaire de l’épopée homérique et la théorie de l’oralité poétique|lieu=Amsterdam|éditeur=Gieben|année=1997}}
* {{Ouvrage|langue=fr|titre=Meillet aujourd'hui|prénom1=Gabriel|nom1=Bergounioux|prénom2=Charles|nom2=de Lamberterie|lieu=Louvain-Paris|éditeur=Peeters|année=2006|isbn=978-9-042-91743-9}}

=== Articles connexes ===
* [[Franz Bopp]]
* [[Johann Kaspar Zeuss]]

=== Liens externes ===
* {{Autorité}}
* {{Dictionnaires}}
* {{Bases recherche}}

{{Portail|linguistique}}

{{DEFAULTSORT:Meillet, Antoine}}
[[Catégorie:Académie des inscriptions et belles-lettres]]
[[Catégorie:Linguiste français]]
[[Catégorie:Philologue français]]
[[Catégorie:Slaviste]]
[[Catégorie:Naissance en novembre 1866]]
[[Catégorie:Naissance à Moulins (Allier)]]
[[Catégorie:Décès en septembre 1936]]
[[Catégorie:Décès dans le Cher]]
[[Catégorie:Décès à 69 ans]]
[[Catégorie:Institut national des langues et civilisations orientales]]
[[Catégorie:Arménologue français]]
[[Catégorie:Indo-européaniste]]
[[Catégorie:Étudiant de l'université de Paris]]
[[Catégorie:Personnalité inhumée à Moulins]]</text>
      <sha1>gbdi9oem6qrnna03l5jvy7wr1sq9a6j</sha1>
    </revision>
  </page>
  <page>
    <title>Algèbre linéaire</title>
    <ns>0</ns>
    <id>7</id>
    <revision>
      <id>154775958</id>
      <parentid>154775946</parentid>
      <timestamp>2018-12-13T19:46:49Z</timestamp>
      <contributor>
        <username>Louis H. G.</username>
        <id>1678379</id>
      </contributor>
      <comment>Annulation de la [[Special:Diff/154775946|modification]] de [[Special:Contributions/105.104.136.223|105.104.136.223]] ([[User talk:105.104.136.223|d]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">

        [[Algèbre générale|alge]]{{Voir homonymes|Algèbre (homonymie)}}
[[Fichier:Linear subspaces with shading.svg|vignette|'''R&lt;sup&gt;3&lt;/sup&gt;''' est un espace vectoriel de dimension 3. Droites et plans qui passent par l'origine sont des sous-espaces vectoriels.]]
L’'''algèbre linéaire''' est la branche des [[mathématiques]] qui s'intéresse aux [[Espace vectoriel|espaces vectoriels]] et aux [[Application linéaire|transformations linéaires]], formalisation générale des théories des [[Système d'équations linéaires|systèmes d'équations linéaires]].

== Histoire ==
L'histoire de l'algèbre linéaire commence avec [[Al-Khawarizmi]] qui a traduit des textes de mathématiques indiens, réinterprété les travaux de l'école grecque et qui est la source du développement conscient de l'algèbre qui s'étendra pendant des siècles après lui&lt;ref&gt;[[Roshdi Rashed]], ''D'Al Khwarizmi à Descartes, Étude sur l'histoire des mathématiques classiques'', Hermann, 2011&lt;/ref&gt;. Elle a été reprise par [[René Descartes]] qui pose des problèmes de [[géométrie]], comme la détermination de l'intersection de deux [[Droite (mathématiques)|droites]], en termes d'[[équation linéaire]], établissant dès lors un pont entre deux branches mathématiques jusqu'alors séparées : l'[[algèbre]] et la géométrie. S'il ne définit pas la notion de base de l'algèbre linéaire qu'est celle d'espace vectoriel, il l'utilise déjà avec succès, et cette utilisation naturelle des aspects linéaires des équations manipulées demeurera utilisée de manière ad hoc, fondée essentiellement sur les idées géométriques sous-jacentes. Après cette découverte, les progrès en algèbre linéaire vont se limiter à des études ponctuelles comme la définition et l'analyse des premières propriétés des [[déterminant (mathématiques)|déterminants]] par [[Jean le Rond D'Alembert|Jean d'Alembert]].

Ce n'est qu'au {{XIXe siècle}} que l'algèbre linéaire devient une branche des mathématiques à part entière. [[Carl Friedrich Gauss]] trouve [[Élimination de Gauss-Jordan|une méthode générique]] pour la résolution des systèmes d'équations linéaires et [[Camille Jordan (mathématicien)|Camille Jordan]] résout définitivement le problème de la [[réduction d'endomorphisme]]. En 1843, [[William Rowan Hamilton]] (inventeur du terme ''vector'') découvre les [[quaternion]]s ([[Algèbre centrale simple|extension]] de degré 4 du [[corps commutatif|corps]] des [[nombre réel|nombres réels]]). En 1844, [[Hermann Günther Grassmann|Hermann Grassmann]] publie son traité ''Die lineale Ausdehnungslehre'', ''La théorie de l'extension linéaire'', qui est la première tentative de formalisation générale de la notion d'espace vectoriel. Si son œuvre reste grandement inaperçue, elle contient l'essentiel des idées modernes de l'algèbre linéaire, et cette étape fondamentale dans le développement de l'algèbre linéaire est reconnue comme telle tant par Hamilton que par [[Giuseppe Peano]], qui axiomatise entièrement la théorie en 1888. Les espaces vectoriels deviennent alors une structure générale omniprésente dans presque tous les domaines mathématiques, notamment en [[analyse (mathématiques)|analyse]] ([[espace fonctionnel|espaces de fonctions]]).

== Intérêt ==
Sous leur forme la plus simple, les applications linéaires dans les [[Espace vectoriel|espaces vectoriels]] représentent intuitivement les déplacements dans les espaces géométriques élémentaires comme la [[Droite (mathématiques)|droite]], le [[Plan (mathématiques)|plan]] ou notre [[Espace (notion)#Physique|espace]] physique. Les bases de cette théorie remplacent maintenant la représentation construite par [[Euclide]] au {{IIIe siècle av. J.-C.}}. La construction moderne permet de généraliser la notion d'espace à des dimensions quelconques.

L'algèbre linéaire permet de résoudre tout un ensemble d'équations dites linéaires utilisées non seulement en mathématiques ou en [[Mécanique (science)|mécanique]], mais aussi dans de nombreuses autres branches comme les [[sciences naturelles]] ou les [[sciences sociales]].

Les espaces vectoriels forment aussi un outil fondamental pour les [[sciences de l'ingénieur]] et servent de base à de nombreux domaines dans la [[recherche opérationnelle]].

Enfin, c'est un outil utilisé en mathématiques dans des domaines aussi divers que la [[théorie des groupes]], [[Théorie des anneaux|des anneaux]] ou [[Théorie des corps|des corps]], l'[[Analyse fonctionnelle (mathématiques)|analyse fonctionnelle]], la [[géométrie différentielle]] ou la [[théorie des nombres]].

== Présentation élémentaire ==
L'algèbre linéaire commence par l'étude de [[vecteur]]s dans les espaces cartésiens de dimension 2 et 3. Un vecteur, ici, est une classe d'équivalence de bipoints qui unifie les segments de droite caractérisés à la fois par leur longueur (ou ''norme''), leur direction et leur sens : deux bipoints représentent un même vecteur si le quadrilatère formé sur les quatre points est un parallélogramme. Les vecteurs peuvent alors être utilisés pour représenter certaines entités physiques comme des déplacements, additionnés entre eux ou encore multipliés par des scalaires (''nombres''), formant ainsi le premier exemple concret d'espace vectoriel.

L'algèbre linéaire moderne s'intéresse beaucoup aux espaces de [[Dimension d'un espace vectoriel|dimension arbitraire, éventuellement infinie]]. La plupart des résultats obtenus en dimension 2 ou 3 peuvent être étendus aux dimensions finies supérieures. Les vecteurs étant des listes ordonnées à n composantes, on peut manipuler ces données efficacement dans cet environnement. Par exemple en [[Sciences économiques|économie]], on peut créer et utiliser des vecteurs à huit dimensions pour représenter le [[produit national brut]] de huit pays.

== Quelques théorèmes ==
* [[Théorème de la base incomplète]]&lt;ref&gt;Dans le cas où ''G ''est infinie, ce théorème utilise l'[[axiome du choix]].&lt;/ref&gt; : soient ''E ''un espace vectoriel, ''G ''une [[famille génératrice]] de ''E ''et ''L ''une [[famille libre]] de vecteurs de ''E''. Alors il existe au moins une [[base (algèbre linéaire)|base]] de ''E ''formée en prenant la réunion de ''L ''et d'une partie de ''G''.
* En particulier, tout espace vectoriel possède au moins une base.
* Toutes les bases d'un même espace vectoriel ont le même [[Cardinalité (mathématiques)|cardinal]].
* Tout espace vectoriel A possède un [[espace dual]] A* ; si A est [[Espace vectoriel de dimension finie|de dimension finie]], A* est de même dimension.
* [[Formule de Grassmann]] : Soient &lt;math&gt;F&lt;/math&gt; et &lt;math&gt;G&lt;/math&gt; deux [[Sous-espace vectoriel|sous-espaces vectoriels]] d'un même espace vectoriel. On a alors :&lt;!--MERCI DE NE PAS MODIFIER LES SIGNES DE CETTE FORMULE : en cas de doute, consulter l'article &quot;Formule de Grassmann&quot;--&gt;&lt;center&gt;&lt;math&gt;\dim(F)+\dim(G)=\dim(F+G)+\dim(F\cap G).&lt;/math&gt;&lt;/center&gt;

D'autres théorèmes concernent les conditions d'inversion de [[Matrice (mathématiques)|matrices]] de divers types :
* [[matrice diagonale]] ;
&lt;!--* bande ?--&gt;
* [[matrice triangulaire]] ;
* [[matrice à diagonale dominante]] {{refsou|date=mars 2013|(très utilisées en analyse numérique)}}.

Un théorème intéressant à l'époque des mémoires d'ordinateurs de petite taille était qu'on pouvait travailler séparément sur des sous-ensembles (« blocs ») d'une matrice en les combinant ensuite par les mêmes règles qu'on utilise pour combiner des scalaires dans les matrices (''cf''. l’article [[Matrice par bloc]]). Avec les mémoires actuelles de plusieurs [[gigaoctet]]s, cette question a perdu un peu de son intérêt pratique, mais reste très prisée en [[théorie des nombres]], pour la [[décomposition en produit de facteurs premiers]] avec le [[crible général de corps de nombres (GNFS)]] (''[[Algorithme de Lanczos|méthode Lanczos]] [[Matrice par bloc|par blocs]]'').

== Utilisations ==
Les espaces vectoriels forment le support et le fondement de l'algèbre linéaire. Ils sont aussi présents dans de nombreux domaines distincts. S'il n'est pas possible d'indiquer ici tous les cas d'utilisation, on peut tout de même citer pour les principales structures objet de théories, des exemples significatifs. Leurs rôles dans de vastes théories ne traitant pas d'une structure particulière, comme celles des [[théorie algébrique des nombres|nombres algébriques]] ou  de [[Théorie de Galois|Galois]] peuvent aussi être évoqués.

Les espaces vectoriels utilisés sont d'une grande diversité. On y trouve les classiques espaces vectoriels de dimension 2 ou 3 sur les [[nombre réel|nombres réels]], cependant la dimension peut être quelconque, même infinie. Les nombres complexes sont aussi très utilisés, ainsi que les [[nombre rationnel|rationnels]]. Il n'est pas rare qu'une partie des nombres réels ou complexes soit considéré comme un espace vectoriel rationnel. Le corps de base peut aussi contenir un nombre fini d'éléments, définissant parfois un [[espace vectoriel fini]].

Les propriétés géométriques de la structure permettent la démonstration de nombreux théorèmes. Elles ne se limitent pas aux cas où l'espace est réel, même dans le cas de corps plus insolites comme les [[corps fini]]s ou les [[extension finie|extensions finies]] des rationnels, les propriétés géométriques s'avèrent parfois essentielles.

=== Groupe fini ===
{{Article détaillé|Représentations d'un groupe fini}}
[[Fichier:Rotations du cube.jpg|thumb|[[Représentations du groupe symétrique|Représentation du groupe symétrique d'indice 4]] comme groupe des rotations du cube dans un espace vectoriel de dimension 3.]]

La [[Groupe fini#Classification des groupes finis|classification des groupes finis]] est une vaste question, encore objet de recherche. Si le groupe contient un petit nombre d'éléments, les [[théorèmes de Sylow]] peuvent suffire pour en déterminer la structure. Une méthode beaucoup plus puissante est nécessaire dans le cas général.

[[Ferdinand Georg Frobenius|Georg Frobenius]], à la suite de travaux de [[Richard Dedekind]], développe une nouvelle théorie&lt;ref&gt;{{en}} [[Charles W. Curtis|C. W. Curtis]], « Representation theory of finite groups, from Frobenius to Brauer », dans ''[[The Mathematical Intelligencer|Math. Intelligencer]]'', 1992, p. 48-57&lt;/ref&gt; en [[1896 en science|1896]]. Elle se fonde sur l'idée que l'ensemble des [[symétrie]]s d'un espace vectoriel possède une structure de groupe. Il est toujours possible de ''représenter'' un groupe fini par des symétries bien choisies sur un espace vectoriel de dimension suffisante. Un groupe est ainsi incarné par des transformations géométriques simples. Une telle incarnation prend le nom de ''représentation d'un groupe''.

Les espaces vectoriels choisis sont de dimension finie, en général sur le corps des complexes&lt;ref&gt;Les 11 premiers chapitres de {{Serre2}} ne concernent que les espaces vectoriels complexes.&lt;/ref&gt;, cependant pour disposer de bonnes propriétés arithmétiques le corps peut être celui des [[nombre rationnel|rationnels]]&lt;ref&gt;{{en}} [[Walter Feit]], ''Characters of finite groups'', Benjamin, 1967&lt;/ref&gt; ou encore utiliser des [[entier algébrique|entiers algébriques]] comme pour la démonstration du [[Théorème de Burnside (groupe résoluble)|théorème de Burnside sur les groupes résolubles]]&lt;ref&gt;{{en}} [[William Burnside]], ''Theory of Groups of Finite Order'', Dover, 2004&lt;/ref&gt;. [[Richard Brauer]] étudie un cas très abstrait, celui des représentations sur un espace vectoriel construit à l'aide d'un [[corps fini]]&lt;ref&gt;{{de}} [[Richard Brauer]], « Über die Darstellung von Gruppen in Galoisschen Feldern », dans ''[[Hermann (éditions)#1876-1956|Act. Sci. Ind.]]'', vol. 195, 1935&lt;/ref&gt;.

Un exemple relativement simple d'utilisation de cette théorie est donné par Burnside, avec [[Théorème de Burnside (problème de 1902)|son théorème]] sur les [[sous-groupe]]s d'[[Exposant d'un groupe|exposant]] fini du [[groupe (mathématiques)|groupe]] [[Groupe général linéaire|linéaire]] GL(''n'', [[nombre complexe|ℂ]]).

=== Anneau ===
{{Article détaillé|Théorie des anneaux}}
[[Fichier:Noether.jpg|thumb|left|[[Emmy Noether]] utilise la notion d'espace vectoriel pour étudier les [[Anneau noethérien|anneaux]] portant maintenant son nom.]]
Un exemple célèbre d'anneau disposant aussi d'une structure d'espace vectoriel est celui des [[polynôme formel|polynômes]] à coefficients dans un corps. Cet espace vectoriel, de dimension infinie, est largement utilisé en algèbre linéaire, à travers par exemple le [[polynôme minimal d'un endomorphisme|polynôme minimal]] ou [[polynôme caractéristique|caractéristique]]. Le [[polynôme d'endomorphisme|morphisme canonique]] entre les polynômes et les applications linéaires d'un espace vectoriel est à l'origine d'une structure d'algèbre qui est un anneau, si la multiplication externe est ''oubliée''.

Cette méthode permet d'élucider la structure de certains anneaux. Tout anneau est un espace vectoriel sur ceux de ses sous-anneaux qui sont des corps. L'espace vectoriel ressemble à la structure développée par Grassman. Cette remarque est utilisée au début du {{s-|XX}}, en particulier par [[Emil Artin]] et [[Emmy Noether]], pour élucider cette structure dans le cas des anneaux artiniens et [[Anneau noethérien|noethériens]], qui sont des copies de sous-algèbres sur un espace vectoriel construit sur sous-anneau qui s'avère être un corps.

Un exemple est la généralisation d'un théorème de [[Joseph Henry Maclagan Wedderburn|Wedderburn]] par Artin et portant maintenant le nom de [[théorème d'Artin-Wedderburn]]. Il est important en [[algèbre non commutative]].

[[Anneau opposé|Un lemme élémentaire]] permet par ailleurs d'interpréter le corps des [[quaternion]]s comme l'algèbre des [[Représentation de groupe#Définitions|endomorphismes]] d'[[Quaternion#Représentation des quaternions comme matrices 4x4 de nombres réels|une représentation réelle de degré 4]] du [[groupe des quaternions|groupe associé]].

=== Théorie de Galois ===
{{Article détaillé|Théorie de Galois}}
[[Fichier:Pentagone construit.png|thumb|La théorie de Galois permet de déterminer quels polygones réguliers sont constructibles à la règle et au compas. [[Construction du pentagone régulier à la règle et au compas|Le pentagone en fait partie]].]]
La théorie de Galois contient de nombreux exemples d'espaces vectoriels. Elle consiste à étudier un corps comme un espace vectoriel sur un sous-corps. Ainsi chaque sous-corps permet de considérer la structure initiale comme un espace vectoriel particulier.

Un exemple d'application est celui des figures [[construction à la règle et au compas|constructible à la règle et au compas]]. Ces points forment un corps disposant d'une structure d'espace vectoriel sur les nombres rationnels. Il est de dimension infinie et, pour chaque point, le plus petit sous-corps le contenant est de dimension finie égale à une puissance de 2. Un tel sous-corps est appelé une [[tour d'extensions quadratiques]]. Cette propriété de ces espaces vectoriels permet de résoudre d'antiques conjectures comme la [[duplication du cube]], la [[trisection de l'angle]] ou la construction d'un [[polygone régulier]].

L'exemple historique de la théorie est celui de la résolution d'une [[équation algébrique|équation polynomiale]]. Le [[Théorème d'Abel (algèbre)|théorème d'Abel]] donne une condition nécessaire et suffisante de résolution par [[Racine d'un nombre#Racines d'un complexe|radicaux]]. Les espaces vectoriels utilisés ont pour éléments ceux du plus petit corps ''L'' contenant tous les coefficients du polynôme ainsi que ses racines et le corps sous-jacent est un sous-corps ''K'' du premier contenant tous les coefficients. Le [[groupe de Galois]] est composé des automorphismes du corps ''L'' et laissant invariant le corps ''K''. Il correspond à un nombre fini de [[symétrie]]s de l'espace vectoriel. {{Refnec|date=mars 2013|L'élément clé de la démonstration montre que l'équation est résoluble seulement si ces symétries sont [[Diagonalisation|diagonalisables]].}}

==Notes et références==

{{Références}}

== Bibliographie ==
* Vincent Blanloeil: ''Une introduction moderne à l’algèbre linéaire''. Ellipses, 2012.
* Roger Mansuy, Rached Mneimné: ''Algèbre linéaire - Réduction des endomorphismes''. Vuibert, 2012.

== Voir aussi ==
{{Autres projets
|commons=Category:Linear algebra
|wikt=algèbre linéaire
|b=Algèbre linéaire
|v=Algèbre linéaire
}}
===Articles connexes===
* [[Propriétés métriques des droites et plans]]
* [[Algèbre multilinéaire]]
* [[Loi d'inertie de Sylvester]]
=== Liens externes ===
* {{en}} [http://www.egwald.com/linearalgebra/index.php Linear Algebra] par Elmer G. Wiens
* [https://web.archive.org/web/20101108172830/http://roso.epfl.ch/teaching.html Les cours du ROSO, dont de l'Algèbre linéaire]
* [http://braise.univ-rennes1.fr/  Braise : la base raisonnée d'exercices de mathématiques et son chapitre sur l'Algèbre linéaire]
* {{lien web
 | url = https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab
 | titre = Essence of linear algebra
 | auteur = 3Blue1Brown
 | site = YouTube
 | consulté le = 2018-06-12
 | langue = en
}} {{commentaire biblio|chaîne dont le but est  « d'animer les intuitions géométriques soustendant de nombreux sujets enseignés dans les crours habituels d'algèbre linéaire. »}}

{{Palette|Algèbre linéaire|Domaines des mathématiques}}
{{Portail|Algèbre}}

{{DEFAULTSORT:Algebre lineaire}}
[[Catégorie:Algèbre linéaire| ]]</text>
      <sha1>q3i758bd4sycrgt7mgwl9xb4olpi8yp</sha1>
    </revision>
  </page>
  <page>
    <title>Algèbre générale</title>
    <ns>0</ns>
    <id>9</id>
    <revision>
      <id>154258553</id>
      <parentid>154258550</parentid>
      <timestamp>2018-11-25T19:58:06Z</timestamp>
      <contributor>
        <username>Salebot</username>
        <id>173239</id>
      </contributor>
      <comment>bot : révocation de [[Special:Contributions/37.165.178.1|37.165.178.1]] (modification suspecte : -25), retour à la version 150731509 de Les Yeux Noirs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">[[Algorithmique|algo]] {{Voir homonymes|Algèbre (homonymie)}}
{{Ébauche|algèbre}}
L''''algèbre générale''', ou '''algèbre abstraite''', est la branche des [[mathématiques]] qui porte principalement sur l'étude des [[structure algébrique|structures algébriques]] et de leurs relations. L'appellation ''algèbre générale'' s'oppose à celle d'''[[Algèbre (mathématiques élémentaires)|algèbre élémentaire]]'' ; cette dernière enseigne le [[calcul algébrique]], c'est-à-dire les règles de manipulation des [[équation algébrique|formules]] et des [[expressions algébriques]].

Historiquement, les structures algébriques sont apparues dans différents domaines des mathématiques, et n'y ont pas été étudiées séparément. C'est pourquoi l'algèbre générale possède beaucoup de connexions avec toutes les branches des mathématiques.

L'étude des structures algébriques peut être faite de manière abstraite, mais unifiée dans le cadre de l'[[algèbre universelle]].

== Histoire ==
Comme dans d'autres parties des mathématiques, des problèmes et des exemples concrets ont joué un rôle important dans le développement de l'algèbre abstraite. Jusqu'à la fin du {{s-|XIX}}, beaucoup - ou plus - de ces problèmes étaient en quelque sorte liés à la théorie des [[Équation algébrique|équations algébriques]]. Les principaux thèmes sont les suivants:
* Résolution de systèmes d'[[Équation linéaire|équations linéaires]], ce qui a conduit à l'[[algèbre linéaire]]
* Tentatives de trouver des formules aux solutions d'[[Équation polynomiale|équations polynomiales]] générales de degré supérieur qui ont abouti à la découverte de [[Groupe de Galois|groupes]] comme des manifestations abstraites de [[symétrie]]
* Études arithmétiques des formes de degré quadratique supérieur et des [[Équation diophantienne|équations diophantiennes]], qui ont produit directement les notions d'un [[Anneau (mathématiques)|anneau]] et [[Idéal (mathématiques)|idéal]].

=== Algèbre moderne ===
La fin du {{s-|XIX}} et le début du {{s-|XX}} a connu un énorme changement dans la méthodologie des mathématiques. L'algèbre abstraite a émergé autour du début du {{s-|XX}}, sous le nom de l'''algèbre moderne''. Son étude faisait partie de l'entraînement pour plus de [[rigueur intellectuelle]] en mathématiques. Les définitions officielles de certaines [[Structure algébrique|structures algébriques]] ont émergé au {{s-|XIX}}.

== Applications ==
En raison de sa généralité, l'algèbre abstraite est utilisée dans de nombreux domaines des mathématiques et de la science. Par exemple, la [[topologie algébrique]] utilise des objets algébriques pour son étude. La [[théorie algébrique des nombres]] étudie divers anneaux numériques qui généralisent l'ensemble des entiers. En utilisant la théorie des nombres algébriques, [[Andrew Wiles]] a prouvé le [[dernier théorème de Fermat]].

== Bases ==
* [[Théorie des ensembles]]
** [[Ensemble|Notion d'ensemble]]
** [[Sous-ensemble]]
** [[Opérations sur les ensembles]]
** [[Produit cartésien]]
*Correspondances et relations
** [[Relation binaire]]
** [[Fonction et application|Fonctions et applications]]
* [[Loi de composition]]
** [[Loi de composition interne|Loi interne]]

== Structures algébriques ==
{{article détaillé|Structure algébrique}}
* [[Magma (mathématiques)|Magma]]
* [[Demi-groupe]] (ou semi-groupe)
* [[Quasigroupe]]
* [[Monoïde]]
* [[Groupe (mathématiques)|Groupe]]
* [[Anneau unitaire|Anneau]]
* [[Module sur un anneau|Module]]
* [[Corps (mathématiques)|Corps]]
* [[Corps commutatif]]
* [[Espace vectoriel]]
* [[Algèbre sur un anneau]]
* [[Algèbre sur un corps]]
* [[Opérade]]

== Articles connexes ==
* [[Évariste Galois]] et [[Niels Henrik Abel]] (mathématiciens ayant fourni un travail majeur pour la construction de l'algèbre)
* [[Emmy Noether]]
* [[Théorie des codes]]
* [[Théorie des groupes]]
{{Palette Domaines des mathématiques}}

{{Portail|algèbre}}

{{DEFAULTSORT:Algebre generale}}

[[Catégorie:Algèbre générale| ]]</text>
      <sha1>5onrmiyf5baevwcesesehi0uqyltg7e</sha1>
    </revision>
  </page>
  <page>
    <title>Algorithmique</title>
    <ns>0</ns>
    <id>10</id>
    <revision>
      <id>155735604</id>
      <parentid>155723994</parentid>
      <timestamp>2019-01-12T09:10:54Z</timestamp>
      <contributor>
        <username>(:Julien:)</username>
        <id>1012</id>
      </contributor>
      <minor />
      <comment>deux liens trouvés ne font pas &quot;le Québec&quot;, qd on cherche dans google on trouve forcément des liens…</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">
        [[Antoine Meillet|antoine]]
        [[Algèbre linéaire|alge]]
        [[Algèbre générale|algegene]]
        [[Fichier:Euclid flowchart 1.png|vignette|[[Organigramme de programmation]] représentant l'[[algorithme d'Euclide]].]]
L{{'}}'''algorithmique''' est l'étude et la production de règles et techniques qui sont impliquées dans la définition et la conception d'[[algorithme]]s, c'est-à-dire de processus systématiques de résolution d'un problème permettant de décrire précisément des étapes pour résoudre un [[problème algorithmique]].

== Étymologie ==
Le mot « algorithme » vient du nom du [[mathématicien]] [[Al-Khwârizmî]]&lt;ref&gt;{{Lien web|auteur1=Phillipe Collard|auteur2=[[Philippe Flajolet]]|titre=Algorithmique|url=http://www.universalis.fr/encyclopedie/algorithmique/|date=|site=Encyclopædia universalis|consulté le=8 mars 2015}}.&lt;/ref&gt; (latinisé au Moyen Âge en {{lang|la|''Algoritmi''}}), qui, au {{IXe siècle}} écrivit [[Abrégé du calcul par la restauration et la comparaison|le premier ouvrage systématique]] donnant des solutions aux [[équation linéaire|équations linéaires]] et [[équation du second degré|quadratiques]]. « Algorithme » a donné « algorithmique ». On trouve aussi le synonyme « algorithmie », vieux mot utilisé par exemple par Hoéné de Wronski en 1811&lt;ref&gt;{{ouvrage|auteur=[[Josef Hoëné-Wronski|Hoéné de Wronski]]|titre=Introduction à la philosophie des mathématiques et technie de l'algorithmie|éditeur=Chez Courcier, imprimeur-libraire pour les mathématiques|date=1811|adresse=quai des Augustin, n°57, Paris|lire en ligne = https://gallica.bnf.fr/ark:/12148/bpt6k6225961k.r=algorithmie}}&lt;/ref&gt;. Certains&lt;ref&gt;Par exemple, l'[[Uqam|UQAM]] propose un cours [https://etudier.uqam.ca/cours?sigle=EDM4600 Algorithmie de base et interactivité] et l'Université de Montréal propose un cours [https://studium.umontreal.ca/course/info.php?id=48463 Algorithmie et effets audionumériques].&lt;/ref&gt;, préfèrent redonner vie à ce mot désuet.

== Histoire ==
[[File:Cuneiform tablet- fragment of a mathematical problem text MET ME86 11 404.jpg|thumb| Fragment d'une tablette cunéiforme avec un problème  algorithmique.  MET ME86 11 404]]
=== Antiquité ===
Les premiers algorithmes dont on a retrouvé des descriptions datent des [[Babylone|Babyloniens]], au {{-millénaire|III|e}}. Ils décrivent des méthodes de [[Calcul (mathématiques)|calcul]] et des résolutions d'équations à l'aide d'exemples&lt;ref&gt;{{article|auteur=[[Donald Knuth]]|périodique=[[Communications of the ACM]]|titre=Ancient Babylonian Algorithms|volume=15|numéro=7|date=juillet 1972}}, repris dans {{ouvrage|auteur=[[Donald Knuth]]|page=185|éditeur=[[Addison-Wesley]]|titre=Selected Papers on Computer Science|date=1996}}, traduit français sous le titre ''Algoritmes babyloniens anciens'' dans {{ouvrage|langue=français|auteur=[[Donald Knuth]]|titre=Éléments pour une histoire de l'informatique|année=2011|éditeur=[[Librairie Eyrolles]]|traducteur= P. Cégielski}}.&lt;/ref&gt;.

Un algorithme célèbre est celui qui se trouve dans le {{nobr|livre 7}} des ''[[algorithme d'Euclide|Éléments d'Euclide]]'', et appelé [[algorithme d'Euclide]]. Il permet de trouver le plus grand diviseur commun, ou [[Plus grand commun diviseur|PGCD]], de deux nombres. Un point particulièrement remarquable est qu’il contient explicitement une [[itération]] et que les {{nobr|propositions 1}} et 2 démontrent sa [[correction d'un algorithme|correction]].

C'est [[Archimède]] qui proposa le premier un algorithme pour le calcul de {{math|[[π]]}}&lt;ref&gt;Le calcul de {{math|π}} {{citation|est caractéristique des problèmes généraux rencontrés en algorithmique.}} {{Lien web|auteur1=Phillipe Collard|auteur2=Phillipe Flajolet|titre=Algorithmique|sous-titre=1. L'exemple du calcul de {{math|π}}|url=http://www.universalis.fr/encyclopedie/algorithmique/1-l-exemple-du-calcul-de-p|date=|site=[[Encyclopædia universalis]]|consulté le=8 mars 2015}}.&lt;/ref&gt;.

=== Étude systématique ===
Le premier à avoir systématisé des algorithmes est le mathématicien [[perse]] [[Al-Khwârizmî]], actif entre 813 et 833. Dans son ouvrage ''[[Abrégé du calcul par la restauration et la comparaison]]'', il étudie toutes les [[équation du second degré|équations du second degré]] et en donne la résolution par des algorithmes généraux. Il utilise des méthodes semblables à celles des [[Mathématiques babyloniennes|Babyloniens]], mais se différencie par ses explications systématiques là où les Babyloniens donnaient seulement des exemples.

Le savant [[Al-Andalus|andalou]] [[Averroès]] ([[1126]]-[[1198]]) évoque une méthode de raisonnement où la thèse s’affine étape par étape, itérativement, jusqu’à une certaine convergence et ceci conformément au déroulement d’un algorithme. À la même époque, au {{XIIe siècle}}, le moine [[Adelard de Bath]] introduit le terme [[latin]] de {{lang|la|''algorismus''}}, par référence au nom de Al Khuwarizmi. Ce mot donne ''algorithme'' en [[français]] en [[1554]].

Au {{XVIIe siècle}}, on pourrait entrevoir une certaine allusion à la méthode algorithmique chez [[René Descartes]] dans la méthode générale proposée par le [[Discours de la méthode]] ([[1637]]), notamment quand, en sa deuxième partie, le mathématicien français propose de {{citation|diviser chacune des difficultés que j’examinerois, en autant de parcelles qu’il se pourroit, et qu’il seroit requis pour les mieux résoudre.}} Sans évoquer explicitement les concepts de boucle, d’itération ou de dichotomie, l’approche de Descartes prédispose la logique à accueillir le concept de programme, mot qui naît en français en [[1677]].

En 1843 , la mathématicienne et pionnière des sciences informatique [[Ada Lovelace]], fille de [[Lord Byron]] et assistante de [[Charles Babbage]] réalise la première implémentation d'un algorithme sous forme de programme (calcul des nombres de Bernoulli)&lt;ref&gt; [[Stephen Wolfram]] {{Lien web|langue=en|url =http://blog.stephenwolfram.com/2015/12/untangling-the-tale-of-ada-lovelace/|titre=Untangling the Tale of Ada Lovelace|site=http://blog.stephenwolfram.com}} &lt;/ref&gt;.

Le [[dixième problème de Hilbert]] qui fait partie de la liste des {{nobr|23 [[Problèmes de Hilbert|problèmes]]}} posés par [[David Hilbert]] en 1900 à Paris est clairement un problème algorithmique. En l'occurrence, la réponse est qu'il n'y a pas d'algorithme répondant au problème posé.

=== L'époque contemporaine ===

L’algorithmique des {{s2-|XX|e|XXI}} a pour fondement mathématique des formalismes, par exemple celui des [[machines de Turing]], qui permettent de définir précisément ce qu'on entend par « étapes », par « précis » et par « non ambigu » et qui donnent un cadre scientifique pour étudier les propriétés des algorithmes. Cependant, suivant le formalisme choisi on obtient des approches algorithmiques différentes pour résoudre un même problème. Par exemple l'[[Algorithme récursif|algorithmique récursive]], l'[[algorithme parallèle|algorithmique parallèle]] ou l’[[informatique quantique]] donnent lieu à des présentations d'algorithmes différentes de celles de l'algorithmique itérative.

Grâce à l'informatique, l'algorithmique s'est beaucoup développée dans la deuxième moitié du {{s-|XX}}. [[Donald Knuth]], auteur du traité ''[[The Art of Computer Programming]]'', qui décrit de très nombreux algorithmes, a contribué, avec d'autres, à en poser les fondements mathématiques de leur analyse.

== Vocabulaire ==
Le substantif ''algorithmique'' désigne l'ensemble des méthodes permettant de créer des algorithmes. Le terme est également employé comme adjectif.

Un ''algorithme'' énonce une solution à un problème sous la forme d’un enchaînement d’''opérations à effectuer''.

Les informaticiens utilisent fréquemment l’anglicisme ''implémentation'' pour désigner la mise en œuvre de l'algorithme dans un [[langage de programmation]] . Cette implémentation réalise la transcription des opérations constitutives de l’algorithme et précise la façon dont ces opérations sont invoquées. Cette écriture en langage informatique, est aussi fréquemment désignée par le terme de « ''[[codage (programmation)|codage]]'' »&lt;ref&gt; En [[cryptographie]], le terme codage est utilisé dans un sens différent. &lt;/ref&gt;. On parle de ''« [[code source]] »'' pour désigner le texte, constituant le programme, réalisant l’algorithme. Le ''code'' est plus ou moins détaillé selon le niveau d’abstraction du langage utilisé, de même qu'une recette de cuisine doit être plus ou moins détaillée selon l’expérience du cuisinier.

== Étude formelle ==
De nombreux outils formels ou théoriques ont été développés pour décrire les algorithmes, les étudier, exprimer leurs qualités, pouvoir les comparer :
* ainsi, pour décrire les algorithmes, des structures algorithmiques ont été mises en évidence : structures de contrôle et structures de données ;
* pour justifier de la qualité des algorithmes, les notions de correction, de complétude et de terminaison ont été mises en place ;
* enfin, pour comparer les algorithmes, une théorie de la complexité des algorithmes a été définie.

=== Structures algorithmiques ===

Les concepts en œuvre en algorithmique, par exemple selon l'approche de [[Niklaus Wirth|N. Wirth]] pour les langages les plus répandus (Pascal, C{{etc.}}), sont en petit nombre. Ils appartiennent à deux classes :
* les [[structure de contrôle|structures de contrôle]] :
** séquences,
** conditionnelles,
** boucles ;
* les [[Structure de données|structures de données]] :
** constantes,
** variables,
** tableaux ;
** structures récursives (listes, arbres, graphes).

Ce découpage est parfois difficile à percevoir pour certains langages ([[Lisp]], [[Prolog]]…) plus basés sur la notion de [[algorithme récursif|récursivité]] où certaines structures de contrôle sont implicites et, donc, semblent disparaître.

=== Correction, complétude, terminaison ===

Ces trois notions « correction », « complétude », « terminaison » sont liées, et supposent qu'un algorithme est écrit pour résoudre un problème.

La [[Terminaison d'un algorithme|terminaison]] est l'assurance que l'algorithme terminera en un temps fini. Les preuves de terminaison font habituellement intervenir une fonction entière positive strictement décroissante à chaque « pas » de l'algorithme.

Étant donnée la garantie qu'un algorithme terminera, la preuve de correction doit apporter l'assurance que si l'algorithme termine en donnant une proposition de solution, alors cette solution est correcte — c'est-à-dire qu'elle est effectivement une solution au problème posé.

La preuve de complétude garantit que, pour un espace de problèmes donné, l'algorithme, s'il termine, donnera une solution pour chacune des entrées.

=== Complexité algorithmique ===

{{Article détaillé|Théorie de la complexité des algorithmes}}

Les principales notions mathématiques dans le calcul du coût d’un algorithme précis sont les [[notation de Landau|notions de domination]] (notée ''O(f(n))'', « grand o »), où ''f'' est une [[fonction mathématique]] de ''n'', variable désignant la quantité d’informations (en [[bit]]s, en nombre d’enregistrements{{etc.}}) manipulée dans l’algorithme. En algorithmique on trouve souvent des complexités du type :

{|class=&quot;wikitable&quot;
! Notation
! Type de complexité
|-
|&lt;math&gt;O(1)&lt;/math&gt;
|complexité constante (indépendante de la taille de la donnée)
|-
|&lt;math&gt;O(\log(n))&lt;/math&gt;
|complexité logarithmique
|-
|&lt;math&gt;O(n)&lt;/math&gt;
|complexité linéaire
|-
|&lt;math&gt;O(n \log(n))&lt;/math&gt;
|complexité quasi-linéaire
|-
|&lt;math&gt;O(n^{2})&lt;/math&gt;
|complexité quadratique
|-
|&lt;math&gt;O(n^{3})&lt;/math&gt;
|complexité cubique
|-
|&lt;math&gt;O(n^p)&lt;/math&gt;
|complexité polynomiale
|-
|&lt;math&gt;O(n^{\log(n)})&lt;/math&gt;
|complexité quasi-polynomiale
|-
|&lt;math&gt;O(2^{n})&lt;/math&gt;
|complexité exponentielle
|-
|&lt;math&gt;O(n!)&lt;/math&gt;
|complexité factorielle
|}

Sans entrer dans les détails mathématiques, le calcul de l’efficacité d’un algorithme (sa ''[[Théorie de la complexité (informatique théorique)|complexité algorithmique]]'') consiste en la recherche de deux quantités importantes. La première quantité est l’évolution du nombre d’instructions de base en fonction de la quantité de données à traiter (par exemple, pour un [[algorithme de tri]], il s'agit du nombre de données à trier), que l’on privilégiera sur le temps d'exécution mesuré en secondes (car ce dernier dépend de la machine sur laquelle l'algorithme s'exécute). La seconde quantité estimée est la quantité de mémoire nécessaire pour effectuer les calculs. Baser le calcul de la complexité d’un algorithme sur le temps ou la quantité effective de mémoire qu’un ordinateur particulier prend pour effectuer ledit algorithme ne permet pas de prendre en compte la structure interne de l’algorithme, ni la particularité de l’ordinateur : selon sa charge de travail, la vitesse de son processeur, la vitesse d’accès aux données, l’exécution de l’algorithme (qui peut faire intervenir le hasard) ou son organisation de la mémoire, le temps d’exécution et la quantité de mémoire ne seront pas les mêmes.

Souvent, on examine les performances « au pire », c'est-à-dire dans les configurations telles que le [[complexité en temps|temps d'exécution]] ou l'[[complexité en espace|espace mémoire]] est le plus grand. Il existe également un autre aspect de l'évaluation de l'efficacité d'un algorithme : les performances « en moyenne ». Cela suppose d'avoir un modèle de la répartition statistique des données de l'algorithme, tandis que la mise en œuvre des techniques d'analyse implique des méthodes assez fines de [[analyse combinatoire|combinatoire]] et d'[[Développement asymptotique|évaluation asymptotique]], utilisant en particulier les [[série génératrice|séries génératrices]] et des méthodes avancées d'[[analyse complexe]]. L'ensemble de ces méthodes est regroupé sous le nom de [[combinatoire analytique]].

On trouvera dans l’article sur la [[théorie de la complexité des algorithmes]] d’autres évaluations de la complexité qui vont en général au-delà des valeurs proposées ci-dessus et qui classifient les problèmes algorithmiques (plutôt que les algorithmes) en classes de complexité.

==== Quelques indications sur l’efficacité des algorithmes ====

Souvent, l’efficacité d’un algorithme n’est connue que de manière asymptotique, c’est-à-dire pour de grandes valeurs du paramètre ''n''. Lorsque ce paramètre est suffisamment petit, un algorithme de complexité asymptotique plus grande peut en pratique être plus efficace. Ainsi, pour trier un tableau de {{nombre|30|lignes}} (c’est un paramètre de petite taille), il est inutile d’utiliser un algorithme évolué comme le [[tri rapide]] (l’un des algorithmes de tri asymptotiquement les plus efficaces en moyenne) : l’algorithme de tri le plus simple à écrire sera suffisamment efficace.

Entre deux algorithmes dont la complexité est identique, on cherchera à utiliser celui dont l’occupation mémoire est la plus faible. L’analyse de la complexité algorithmique peut également servir à évaluer l’occupation mémoire d’un algorithme. Enfin, le choix d’un algorithme plutôt qu’un autre doit se faire en fonction des données que l’on s’attend à lui fournir en entrée. Ainsi, le [[tri rapide]], lorsque l’on choisit le premier élément comme pivot, se comporte de façon désastreuse si on l’applique à une liste de valeurs déjà triée. Il n’est donc pas judicieux de l’utiliser si on prévoit que le programme recevra en entrée des listes déjà presque triées ou alors il faudra choisir le pivot aléatoirement.

Un autre paramètre à prendre en compte est la [[Mémoire virtuelle#Principe de localité|localité]] de l’algorithme. Par exemple pour un système à [[mémoire virtuelle]] qui dispose de peu de mémoire vive (par rapport au nombre de données à traiter), le [[tri rapide]] sera normalement plus efficace que le [[tri par tas]] car le premier ne passe qu’une seule fois sur chaque élément de la mémoire tandis que le second accède à la mémoire de manière discontinue (ce qui augmente le risque de {{lang|en|''[[Mémoire virtuelle#Swapping|swapping]]''}}).

Enfin, il existe certains algorithmes dont l'analyse de complexité est dite [[analyse amortie|amortie]]. Cela signifie que, pour certaines exécutions de l’algorithme (cas marginaux), la complexité de l’algorithme sera très supérieure au cas moyen, mais sera compensée par des exécutions rendues efficaces du même algorithme dans une suite d'invocations de cet algorithme.

L'[[Analyse lisse d'algorithme|analyse lisse]] mesure les performances des algorithmes sur les pires cas, mais avec une légère perturbation des instances. Elle permet d'expliquer pourquoi des algorithmes analysés comme inefficaces autrement sont en fait efficaces en pratique. L'[[algorithme du simplexe]] est un exemple d'un algorithme qui se comporte bien pour l'analyse lisse.

== Approches pratiques ==

L'algorithmique a développé quelques stratégies pour résoudre les problèmes :
* [[algorithme glouton]] : un premier algorithme peut souvent être proposé en étudiant le problème très progressivement : on résout chaque sous-problème localement en espérant que l'ensemble de leurs résultats composera bien une solution du problème global. On parle alors d'algorithme glouton. L'algorithme glouton n'est souvent qu'une première étape dans la rédaction d'un algorithme plus performant ;
* [[Diviser pour régner (informatique)|diviser pour régner]] : pour améliorer les performances des algorithmes, une technique usuelle consiste à diviser les données d'un problème en sous-ensembles de tailles plus petites, jusqu'à obtenir des données que l'algorithme pourra traiter au cas par cas. Une seconde étape dans ces algorithmes consiste à « fusionner » les résultats partiels pour obtenir une solution globale. Ces algorithmes sont souvent associés à la récursivité ;
* [[recherche exhaustive]] (ou combinatoire) : une méthode utilisant l'énorme puissance de calcul des ordinateurs consiste à regarder tous les cas possibles. Cela n'est pour autant possible que dans certains cas particuliers (la combinatoire est souvent plus forte que l'énorme puissance des ordinateurs, aussi énorme soit-elle) ;
* décomposition ''top-down'' / ''bottom-up'' : (décomposition descendante, décomposition remontante) les décompositions ''top-down'' consistent à essayer de décomposer le problème en sous-problèmes à résoudre successivement, la décomposition allant jusqu'à des problèmes triviaux faciles à résoudre. L'algorithme global est alors donné par la composée des algorithmes définis au cours de la décomposition. La démarche ''bottom-up'' est la démarche inverse, elle consiste à partir d'algorithmes simples, ne résolvant qu'une étape du problème, pour essayer de les composer pour obtenir un algorithme global ;
* pré-traitement / post-traitement : parfois, certains algorithmes comportent une ou deux phases identifiées comme des pré-traitements (à faire avant l'algorithme principal), ou post-traitement (à faire après l'algorithme principal), pour simplifier l'écriture de l'algorithme général ;
* [[programmation dynamique]] : elle s'applique lorsque le problème d'optimisation est composé de plusieurs sous-problèmes de même nature, et qu'une solution optimale du problème global s'obtient à partir de solutions optimales des sous-problèmes.

=== Les heuristiques ===
{{article détaillé|Algorithme de Las Vegas|Algorithme de Monte-Carlo}}

Pour certains problèmes, les algorithmes ont une complexité beaucoup trop grande pour obtenir un résultat en temps raisonnable, même si l’on pouvait utiliser une puissance de calcul phénoménale. On est donc amené à rechercher la solution de façon non systématique ([[algorithme de Las Vegas]]) ou de se contenter d'une solution la plus proche possible d’une solution optimale en procédant par essais successifs ([[algorithme de Monte-Carlo]]). Puisque toutes les combinaisons ne peuvent être essayées, certains choix stratégiques doivent être faits. Ces choix, généralement très dépendants du problème traité, constituent ce qu’on appelle une [[heuristique (mathématiques)|heuristique]]. Le but d’une heuristique n'est donc pas d'essayer toutes les combinaisons possibles, mais de trouver une solution en un temps raisonnable et par un autre moyen, par exemple en procédant à des tirages aléatoires. La solution peut être exacte (Las Vegas) ou approchée (Monte-Carlo). Les ''algorithmes d'Atlantic City'' quant à eux donnent de façon probablement efficace une réponse probablement juste (disons avec une chance sur cent millions de se tromper) à la question posée.

C’est ainsi que les programmes de [[Échecs|jeu d’échecs]] ou de [[jeu de go]] (pour ne citer que ceux-là) font appel de manière très fréquente à des heuristiques qui modélisent l’expérience d’un joueur. Certains [[Logiciel antivirus|logiciels antivirus]] se basent également sur des heuristiques pour reconnaître des [[virus informatique]]s non répertoriés dans leur base, en s’appuyant sur des ressemblances avec des virus connus, c'est un exemple d'algorithme d'Atlantic City. De même le [[problème SAT]] qui est l'archétype du [[problème NP-complet]] donc très difficile est résolu de [[Problème SAT#Algorithmes de SAT|façon pratique et efficace par la mise au point d'heuristiques]]&lt;ref&gt;{{en}} Moshe Vardi, ''{{Langue|en|Boolean Satisfiability: Theory and Engineering}}'' [http://cacm.acm.org/magazines/2014/3/172516-boolean-satisfiability/fulltext (Communications of the ACM, Vol. 57 No. 3, Page 5)].&lt;/ref&gt;.

== Exemples d’algorithmes, de problèmes, d'applications ou domaines d'application ==

Il existe un certain nombre d’algorithmes classiques, utilisés pour résoudre des problèmes ou plus simplement pour illustrer des méthodes de programmation. On se référera aux articles suivants pour de plus amples détails (voir aussi [[liste des algorithmes]]) :
* algorithmes ou problèmes classiques (du plus simple ou plus complexe) :
** échange, ou comment échanger les valeurs de deux variables : problème classique illustrant la notion de variable informatique (voir aussi [[Structure de données]]),
** algorithmes de recherche, ou comment retrouver une information dans un ensemble structuré ou non (par exemple [[Recherche dichotomique]]),
** [[algorithme de tri]], ou comment trier un ensemble de nombres le plus rapidement possible ou en utilisant le moins de ressources possible,
** [[problème du voyageur de commerce]], [[problème du sac à dos]], [[problème SAT]] et autres algorithmes ou approximations de solutions pour les problèmes combinatoires difficiles (dit NP-complets) ;
* algorithmes ou problèmes illustrant la programmation récursive (voir aussi [[algorithme récursif]]) :
** [[tours de Hanoï]],
** [[huit dames]], placer huit dames sur un échiquier sans qu’elles puissent se prendre entre elles,
** [[suite de Conway]],
** algorithme de dessins récursifs ([[fractale]]) pour le [[Tapis de Sierpiński]], la [[Courbe du dragon]], le [[Flocon de Koch]]… ;
* algorithmes dans le domaine des mathématiques :
** calcul de la [[factorielle]] d'un nombre, de la [[Fonction d'Ackermann]] ou de la [[suite de Fibonacci]],
** [[algorithme du simplexe]], qui minimise une fonction linéaire de variables réelles soumises à des contraintes linéaires,
** [[fraction continue d'un nombre quadratique]], permettant d'extraire une [[racine carrée]], cas particulier de la [[méthode de Newton]],
** dans le domaine de l'algèbre : l'[[unification|algorithme d'unification]], le calcul d'une [[bases de Gröbner|base de Gröbner]] d'un idéal de polynôme et plus généralement presque toutes les méthodes de [[calcul symbolique]],
** en [[Théorie des graphes#Aspect algorithmique|théorie des graphes]] qui donne lieu à de nombreux algorithmes,
** [[test de primalité]] ;
* algorithmes pour et dans le domaine de l'informatique :
** [[cryptologie]] et [[compression de données]],
** [[informatique musicale]],
** [[algorithme génétique]] en [[informatique décisionnelle]],
** analyse et compilation des langages formels (voir [[Compilateur]] et [[Interprète (informatique)]]),
** [[allocation de mémoire]] ([[ramasse-miettes (informatique)|ramasse-miettes]]).

== Annexes ==
{{Autres projets
|wiktionary=algorithmie
|wikiversity=Algorithmique
|wikibooks=Algorithmique impérative}}

=== Notes et références ===
{{Références}}

=== Bibliographie ===
* {{Ouvrage|langue=en|titre=[[The Art of Computer Programming]]|auteur1=[[Donald Knuth|Donald E. Knuth]]|volume=2|titre volume=Seminumerical algorithms|lieu=Reading, Mass|éditeur=Addison-Wesley Pub. Co|année=1973|pages totales=764|isbn=978-0-201-89684-8|isbn2=978-0-321-75104-1|oclc=781024586}}
* {{Algorithmique (Quercia)}}
* {{Cormen3fr}}

=== Liens externes ===
* [http://interstices.info/algo Qu’est-ce qu'un algorithme ?] par [[Philippe Flajolet]] et Étienne Parizot sur la revue en ligne [[Interstices]]

=== Articles connexes ===
* [[Algorithme récursif]]
* [[Algorithme réparti]]
* [[Algorithme émergent]]
* [[Algorithme adaptatif]]
* [[Art algorithmique]]
* [[Liste d'algorithmes]]
* [[Métaheuristique]]
* [[Recherche opérationnelle]]
* [[Paradigme (programmation)]]

{{Palette|Informatique théorique|Domaines de l'informatique}}

{{Portail|informatique théorique}}

[[Catégorie:Algorithmique|*]]</text>
      <sha1>c63xi868b6e74ipvsg1jvqniwvlfty6</sha1>
    </revision>
  </page>


  </mediawiki>